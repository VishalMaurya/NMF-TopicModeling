{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic Modeling Assessment Project\n",
    "\n",
    "A dataset of over 400,000 quora questions that have no labeled cateogry, and attempting to find 20 cateogries to assign these questions to. The .csv file of these text questions can be found underneath the Topic-Modeling folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Import pandas and read in the quora_questions.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('quora_questions.csv',sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404289 entries, 0 to 404288\n",
      "Data columns (total 1 columns):\n",
      "Question    404289 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.columns\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question\n",
       "0  What is the step by step guide to invest in sh...\n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2  How can I increase the speed of my internet co...\n",
       "3  Why am I mentally very lonely? How can I solve...\n",
       "4  Which one dissolve in water quikly sugar, salt..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "####  Use TF-IDF Vectorization to create a vectorized document term matrix. You may want to explore the max_df and min_df parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(max_df=0.95,min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tran = tv.fit_transform(data['Question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<404289x38669 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2002912 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-negative Matrix Factorization\n",
    "\n",
    "#### Using Scikit-Learn create an instance of NMF with 20 expected components. (Use random_state=42).."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF = NMF(n_components=200, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "    n_components=200, random_state=42, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF.fit(data_tran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=20, random_state=42, shuffle=False, solver='cd', tol=0.0001,\n",
       "  verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print our the top 15 most common words for each of the 20 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['thing', 'read', 'place', 'visit', 'places', 'phone', 'buy', 'laptop', 'movie', 'ways', '2016', 'books', 'book', 'movies', 'best']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['majors', 'recruit', 'sex', 'looking', 'differ', 'use', 'exist', 'really', 'compare', 'cost', 'long', 'feel', 'work', 'mean', 'does']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['add', 'answered', 'needing', 'post', 'easily', 'improvement', 'delete', 'asked', 'google', 'answers', 'answer', 'ask', 'question', 'questions', 'quora']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['using', 'website', 'investment', 'friends', 'black', 'internet', 'free', 'home', 'easy', 'youtube', 'ways', 'earn', 'online', 'make', 'money']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['balance', 'earth', 'day', 'death', 'changed', 'live', 'want', 'change', 'moment', 'real', 'important', 'thing', 'meaning', 'purpose', 'life']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['reservation', 'engineering', 'minister', 'president', 'company', 'china', 'business', 'country', 'olympics', 'available', 'job', 'spotify', 'war', 'pakistan', 'india']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['beginners', 'online', 'english', 'book', 'did', 'hacking', 'want', 'python', 'languages', 'java', 'learning', 'start', 'language', 'programming', 'learn']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #7\n",
      "['happen', 'presidency', 'think', 'presidential', '2016', 'vote', 'better', 'election', 'did', 'win', 'hillary', 'president', 'clinton', 'donald', 'trump']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #8\n",
      "['russia', 'business', 'win', 'coming', 'countries', 'place', 'pakistan', 'happen', 'end', 'country', 'iii', 'start', 'did', 'war', 'world']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #9\n",
      "['indian', 'companies', 'don', 'guy', 'men', 'culture', 'women', 'work', 'girls', 'live', 'girl', 'look', 'sex', 'feel', 'like']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #10\n",
      "['ca', 'departments', 'positions', 'movies', 'songs', 'business', 'read', 'start', 'job', 'work', 'engineering', 'ways', 'bad', 'books', 'good']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #11\n",
      "['money', 'modi', 'currency', 'economy', 'think', 'government', 'ban', 'banning', 'black', 'indian', 'rupee', 'rs', '1000', 'notes', '500']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #12\n",
      "['blowing', 'resolutions', 'resolution', 'mind', 'likes', 'girl', '2017', 'year', 'don', 'employees', 'going', 'day', 'things', 'new', 'know']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #13\n",
      "['aspects', 'fluent', 'skill', 'spoken', 'ways', 'language', 'fluently', 'speak', 'communication', 'pronunciation', 'speaking', 'writing', 'skills', 'improve', 'english']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #14\n",
      "['diet', 'help', 'healthy', 'exercise', 'month', 'pounds', 'reduce', 'quickly', 'loss', 'fast', 'fat', 'ways', 'gain', 'lose', 'weight']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #15\n",
      "['having', 'feel', 'long', 'spend', 'did', 'person', 'machine', 'movies', 'favorite', 'job', 'home', 'sex', 'possible', 'travel', 'time']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #16\n",
      "['marriage', 'make', 'did', 'girlfriend', 'feel', 'tell', 'forget', 'really', 'friend', 'true', 'know', 'person', 'girl', 'fall', 'love']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #17\n",
      "['easy', 'hack', 'prepare', 'quickest', 'facebook', 'increase', 'painless', 'instagram', 'account', 'best', 'commit', 'fastest', 'suicide', 'easiest', 'way']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #18\n",
      "['web', 'java', 'scripting', 'phone', 'mechanical', 'better', 'job', 'use', 'account', 'data', 'software', 'science', 'computer', 'engineering', 'difference']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #19\n",
      "['earth', 'blowing', 'stop', 'use', 'easily', 'mind', 'google', 'flat', 'questions', 'hate', 'believe', 'ask', 'don', 'think', 'people']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, topic in enumerate(NMF.components_):\n",
    "    print(f'\\nTHE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([tv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Add a new column to the original quora dataframe that labels each question into one of the 20 topic categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  Topic\n",
       "0  What is the step by step guide to invest in sh...      5\n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...     16\n",
       "2  How can I increase the speed of my internet co...     17\n",
       "3  Why am I mentally very lonely? How can I solve...     11\n",
       "4  Which one dissolve in water quikly sugar, salt...     14"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_tran = NMF.transform(data_tran)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_tran[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = nmf_tran.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 16, 17, 11, 14,  1,  0, 10, 19, 17], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  Topic\n",
       "0  What is the step by step guide to invest in sh...      5\n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...     16\n",
       "2  How can I increase the speed of my internet co...     17\n",
       "3  Why am I mentally very lonely? How can I solve...     11\n",
       "4  Which one dissolve in water quikly sugar, salt...     14"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Topic'] = topic\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying vector on Topic 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=pd.Series( ['money', 'modi', 'currency', 'economy', 'think', 'government', 'ban', 'banning', 'black', 'indian', 'rupee', 'rs', '1000', 'notes', '500'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, nltk\n",
    "import en_core_web_md\n",
    "\n",
    "#nltk.download('vader_lexicon')\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(c):  \n",
    "    return c.apply(lambda x : nlp.vocab[x].vector)\n",
    "\n",
    "\n",
    "def vector_math(word):\n",
    "    \n",
    "    computed_similarities = []\n",
    "    cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    "    \n",
    "    word_vec=exp(word)\n",
    "    \n",
    "    new_vec = sum(word_vec)\n",
    "    for word in nlp.vocab:\n",
    "    # Ignore words without vectors and mixed-case words:\n",
    "        if word.has_vector:\n",
    "            if word.is_lower:\n",
    "                if word.is_alpha:\n",
    "                    similarity = cosine_similarity(new_vec, word.vector)\n",
    "                    computed_similarities.append((word, similarity))\n",
    "\n",
    "    computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "    \n",
    "    result = [w[0].text for w in computed_similarities[:10]]\n",
    "    #print(result)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moolah',\n",
       " 'oney',\n",
       " 'moeny',\n",
       " 'money',\n",
       " 'mone',\n",
       " 'government',\n",
       " 'dollor',\n",
       " 'naira',\n",
       " 'dollar',\n",
       " 'bolivars']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_math(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           games\n",
      "1             seo\n",
      "2            apps\n",
      "3         service\n",
      "4       beginners\n",
      "5              ve\n",
      "6       bangalore\n",
      "7        websites\n",
      "8           15000\n",
      "9           study\n",
      "10         course\n",
      "11          songs\n",
      "12       software\n",
      "13      bollywood\n",
      "14        digital\n",
      "15          sites\n",
      "16       download\n",
      "17      institute\n",
      "18     smartphone\n",
      "19      marketing\n",
      "20        company\n",
      "21          delhi\n",
      "22          watch\n",
      "23         mobile\n",
      "24       learning\n",
      "25        android\n",
      "26       coaching\n",
      "27         online\n",
      "28           free\n",
      "29      hollywood\n",
      "30            app\n",
      "31    engineering\n",
      "32         friend\n",
      "33        website\n",
      "34           site\n",
      "35          thing\n",
      "36           read\n",
      "37          place\n",
      "38          visit\n",
      "39         places\n",
      "40          phone\n",
      "41            buy\n",
      "42         laptop\n",
      "43          movie\n",
      "44           ways\n",
      "45           2016\n",
      "46          books\n",
      "47           book\n",
      "48         movies\n",
      "49           best\n",
      "dtype: object\n",
      "0         actually\n",
      "1            woman\n",
      "2              com\n",
      "3           iphone\n",
      "4           energy\n",
      "5          average\n",
      "6            works\n",
      "7         increase\n",
      "8              guy\n",
      "9             type\n",
      "10            look\n",
      "11           cause\n",
      "12          google\n",
      "13            hair\n",
      "14             new\n",
      "15           water\n",
      "16       positions\n",
      "17     departments\n",
      "18         balance\n",
      "19            word\n",
      "20          matter\n",
      "21        facebook\n",
      "22            help\n",
      "23          symbol\n",
      "24          person\n",
      "25             say\n",
      "26           dream\n",
      "27        distance\n",
      "28            need\n",
      "29          affect\n",
      "30            come\n",
      "31    relationship\n",
      "32            says\n",
      "33    universities\n",
      "34           grads\n",
      "35          majors\n",
      "36         recruit\n",
      "37             sex\n",
      "38         looking\n",
      "39          differ\n",
      "40             use\n",
      "41           exist\n",
      "42          really\n",
      "43         compare\n",
      "44            cost\n",
      "45            long\n",
      "46            feel\n",
      "47            work\n",
      "48            mean\n",
      "49            does\n",
      "dtype: object\n",
      "0         upvoted\n",
      "1            earn\n",
      "2          number\n",
      "3         details\n",
      "4         upvotes\n",
      "5            come\n",
      "6            blog\n",
      "7       followers\n",
      "8          asking\n",
      "9           block\n",
      "10        message\n",
      "11           mark\n",
      "12          doesn\n",
      "13      anonymous\n",
      "14         stupid\n",
      "15          views\n",
      "16           need\n",
      "17      answering\n",
      "18         change\n",
      "19        instead\n",
      "20        picture\n",
      "21          using\n",
      "22         topics\n",
      "23          write\n",
      "24            did\n",
      "25        account\n",
      "26      interview\n",
      "27       googling\n",
      "28        profile\n",
      "29         follow\n",
      "30          users\n",
      "31         writer\n",
      "32         marked\n",
      "33         search\n",
      "34            use\n",
      "35            add\n",
      "36       answered\n",
      "37        needing\n",
      "38           post\n",
      "39         easily\n",
      "40    improvement\n",
      "41         delete\n",
      "42          asked\n",
      "43         google\n",
      "44        answers\n",
      "45         answer\n",
      "46            ask\n",
      "47       question\n",
      "48      questions\n",
      "49          quora\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "topicword = []\n",
    "for topic in NMF.components_[:3]:\n",
    "    data_word =pd.Series([tv.get_feature_names()[i] for i in topic.argsort()[-50:]])\n",
    "    print(data_word)\n",
    "    topicword.append(vector_math(data_word))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['webmaster',\n",
       "  'intranet',\n",
       "  'web',\n",
       "  'onine',\n",
       "  'online',\n",
       "  'onlines',\n",
       "  'onlin',\n",
       "  'intenet',\n",
       "  'internet',\n",
       "  'interent'],\n",
       " ['even',\n",
       "  'because',\n",
       "  'way',\n",
       "  'what',\n",
       "  'that',\n",
       "  'but',\n",
       "  'how',\n",
       "  'could',\n",
       "  'would',\n",
       "  'there'],\n",
       " ['ask',\n",
       "  'answer',\n",
       "  'you',\n",
       "  'sure',\n",
       "  'if',\n",
       "  'someone',\n",
       "  'anyone',\n",
       "  'want',\n",
       "  'what',\n",
       "  'tell']]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicword"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
